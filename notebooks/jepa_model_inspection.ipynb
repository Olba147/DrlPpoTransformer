{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JEPA Model Inspection\n",
        "\n",
        "Inspect a JEPA pretraining config and checkpoint: hyperparameters, parameter counts, and checkpoint metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: C:\\python\\koulu\\Gradu\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    p = start.resolve()\n",
        "    for candidate in [p, *p.parents]:\n",
        "        if (candidate / \"src\").exists() and (candidate / \"configs\").exists():\n",
        "            return candidate\n",
        "    raise RuntimeError(\"Could not locate project root containing src/ and configs/\")\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_project_root(Path.cwd())\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "\n",
        "import sys\n",
        "if str(PROJECT_ROOT / \"src\") not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
        "\n",
        "from models.jepa.jepa import JEPA\n",
        "from models.time_series.patchTransformer import PatchTSTEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# User parameters\n",
        "# -----------------------------\n",
        "JEPA_CONFIG_PATH = \"configs/jepa_pretrain6.json\"\n",
        "\n",
        "# Optional checkpoint override. Set to None to auto-pick best.pt then latest epoch*.pt.\n",
        "JEPA_CHECKPOINT_PATH = None\n",
        "\n",
        "# If True, print first N state_dict keys\n",
        "SHOW_STATE_KEYS = False\n",
        "STATE_KEYS_N = 40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresolve_project_path\u001b[39m(path_value: \u001b[38;5;28mstr\u001b[39m | \u001b[43mPath\u001b[49m) -> Path:\n\u001b[32m      2\u001b[39m     p = Path(path_value)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p.is_absolute():\n",
            "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
          ]
        }
      ],
      "source": [
        "def resolve_project_path(path_value: str | Path) -> Path:\n",
        "    p = Path(path_value)\n",
        "    if p.is_absolute():\n",
        "        return p\n",
        "    return (PROJECT_ROOT / p).resolve()\n",
        "\n",
        "\n",
        "def load_json(path: Path) -> dict:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def pick_checkpoint(checkpoint_dir: Path) -> Path | None:\n",
        "    best_path = checkpoint_dir / \"best.pt\"\n",
        "    if best_path.exists():\n",
        "        return best_path\n",
        "\n",
        "    epoch_ckpts = sorted(checkpoint_dir.glob(\"epoch*.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    return epoch_ckpts[0] if epoch_ckpts else None\n",
        "\n",
        "\n",
        "cfg_path = resolve_project_path(JEPA_CONFIG_PATH)\n",
        "cfg = load_json(cfg_path)\n",
        "\n",
        "model_name = cfg[\"model_name\"]\n",
        "paths_cfg = cfg.get(\"paths\", {})\n",
        "jepa_cfg = cfg[\"jepa_model\"]\n",
        "train_cfg = cfg.get(\"training\", {})\n",
        "dataset_cfg = cfg.get(\"dataset\", {})\n",
        "loss_cfg = cfg.get(\"loss\", {})\n",
        "\n",
        "checkpoint_root = resolve_project_path(paths_cfg.get(\"checkpoint_root\", \"checkpoints\"))\n",
        "checkpoint_dir = checkpoint_root / model_name\n",
        "\n",
        "if JEPA_CHECKPOINT_PATH is not None:\n",
        "    ckpt_path = resolve_project_path(JEPA_CHECKPOINT_PATH)\n",
        "else:\n",
        "    ckpt_path = pick_checkpoint(checkpoint_dir)\n",
        "\n",
        "print(f\"Config: {cfg_path}\")\n",
        "print(f\"Model name: {model_name}\")\n",
        "print(f\"Checkpoint dir: {checkpoint_dir}\")\n",
        "print(f\"Checkpoint selected: {ckpt_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JEPA\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ollik\\miniconda3\\envs\\.graduenv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Build JEPA model from config\n",
        "\n",
        "# Backward-compatible key handling\n",
        "use_asset_embeddings = jepa_cfg.get(\"use_asset_embeddings\", jepa_cfg.get(\"use_asset_embedding\", True))\n",
        "num_assets = None  # dataset-dependent; omitted for architecture inspection by default\n",
        "\n",
        "context_enc = PatchTSTEncoder(\n",
        "    patch_len=jepa_cfg[\"patch_len\"],\n",
        "    d_model=jepa_cfg[\"d_model\"],\n",
        "    n_features=jepa_cfg[\"n_features\"],\n",
        "    n_time_features=jepa_cfg[\"n_time_features\"],\n",
        "    nhead=jepa_cfg[\"nhead\"],\n",
        "    num_layers=jepa_cfg[\"num_layers\"],\n",
        "    dim_ff=jepa_cfg[\"dim_ff\"],\n",
        "    dropout=jepa_cfg[\"dropout\"],\n",
        "    add_cls=jepa_cfg.get(\"add_cls\", True),\n",
        "    pooling=jepa_cfg[\"pooling\"],\n",
        "    pred_len=jepa_cfg[\"pred_len\"],\n",
        "    num_assets=num_assets if use_asset_embeddings else None,\n",
        ")\n",
        "\n",
        "target_enc = copy.deepcopy(context_enc)\n",
        "\n",
        "model = JEPA(\n",
        "    context_enc,\n",
        "    target_enc,\n",
        "    d_model=jepa_cfg[\"d_model\"],\n",
        "    ema_tau_min=jepa_cfg[\"ema_tau_min\"],\n",
        "    ema_tau_max=jepa_cfg[\"ema_tau_max\"],\n",
        ")\n",
        "\n",
        "print(model.__class__.__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>module</th>\n",
              "      <th>params_total</th>\n",
              "      <th>params_trainable</th>\n",
              "      <th>params_total_fmt</th>\n",
              "      <th>params_trainable_fmt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>context_enc</td>\n",
              "      <td>1800385</td>\n",
              "      <td>1800385</td>\n",
              "      <td>1,800,385 (1.800M)</td>\n",
              "      <td>1,800,385 (1.800M)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>target_enc</td>\n",
              "      <td>1800385</td>\n",
              "      <td>0</td>\n",
              "      <td>1,800,385 (1.800M)</td>\n",
              "      <td>0 (0.000M)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>predictor</td>\n",
              "      <td>296256</td>\n",
              "      <td>296256</td>\n",
              "      <td>296,256 (0.296M)</td>\n",
              "      <td>296,256 (0.296M)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>full_model</td>\n",
              "      <td>3897026</td>\n",
              "      <td>2096641</td>\n",
              "      <td>3,897,026 (3.897M)</td>\n",
              "      <td>2,096,641 (2.097M)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        module  params_total  params_trainable    params_total_fmt  \\\n",
              "0  context_enc       1800385           1800385  1,800,385 (1.800M)   \n",
              "1   target_enc       1800385                 0  1,800,385 (1.800M)   \n",
              "2    predictor        296256            296256    296,256 (0.296M)   \n",
              "3   full_model       3897026           2096641  3,897,026 (3.897M)   \n",
              "\n",
              "  params_trainable_fmt  \n",
              "0   1,800,385 (1.800M)  \n",
              "1           0 (0.000M)  \n",
              "2     296,256 (0.296M)  \n",
              "3   2,096,641 (2.097M)  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def count_params(module: torch.nn.Module) -> tuple[int, int]:\n",
        "    total = sum(p.numel() for p in module.parameters())\n",
        "    trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "\n",
        "def format_m(n: int) -> str:\n",
        "    return f\"{n:,} ({n/1e6:.3f}M)\"\n",
        "\n",
        "\n",
        "rows = []\n",
        "for name, module in [\n",
        "    (\"context_enc\", model.context_enc),\n",
        "    (\"target_enc\", model.target_enc),\n",
        "    (\"predictor\", model.predictor),\n",
        "    (\"full_model\", model),\n",
        "]:\n",
        "    total, trainable = count_params(module)\n",
        "    rows.append({\n",
        "        \"module\": name,\n",
        "        \"params_total\": total,\n",
        "        \"params_trainable\": trainable,\n",
        "        \"params_total_fmt\": format_m(total),\n",
        "        \"params_trainable_fmt\": format_m(trainable),\n",
        "    })\n",
        "\n",
        "param_df = pd.DataFrame(rows)\n",
        "param_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>model_name</td>\n",
              "      <td>jepa_initial6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>patch_len</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>patch_stride</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>d_model</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n_features</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>n_time_features</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>nhead</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>num_layers</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>dim_ff</td>\n",
              "      <td>768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>dropout</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>add_cls</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>pooling</td>\n",
              "      <td>cls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>pred_len</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ema_tau_min</td>\n",
              "      <td>0.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>ema_tau_max</td>\n",
              "      <td>0.999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>use_asset_embeddings</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>train_epochs</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>train_batch_size</td>\n",
              "      <td>256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>learning_rate</td>\n",
              "      <td>0.0003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>dataset_root_path</td>\n",
              "      <td>Data/polygon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>dataset_data_path</td>\n",
              "      <td>data_raw_1m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>dataset_timeframe</td>\n",
              "      <td>15min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>loss_type</td>\n",
              "      <td>smoothL1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     key          value\n",
              "0             model_name  jepa_initial6\n",
              "1              patch_len              8\n",
              "2           patch_stride              8\n",
              "3                d_model            192\n",
              "4             n_features              9\n",
              "5        n_time_features              2\n",
              "6                  nhead              4\n",
              "7             num_layers              4\n",
              "8                 dim_ff            768\n",
              "9                dropout            0.0\n",
              "10               add_cls           True\n",
              "11               pooling            cls\n",
              "12              pred_len             48\n",
              "13           ema_tau_min           0.99\n",
              "14           ema_tau_max          0.999\n",
              "15  use_asset_embeddings          False\n",
              "16          train_epochs            200\n",
              "17      train_batch_size            256\n",
              "18         learning_rate         0.0003\n",
              "19     dataset_root_path   Data/polygon\n",
              "20     dataset_data_path    data_raw_1m\n",
              "21     dataset_timeframe          15min\n",
              "22             loss_type       smoothL1"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hyper_rows = [\n",
        "    (\"model_name\", model_name),\n",
        "    (\"patch_len\", jepa_cfg.get(\"patch_len\")),\n",
        "    (\"patch_stride\", jepa_cfg.get(\"patch_stride\")),\n",
        "    (\"d_model\", jepa_cfg.get(\"d_model\")),\n",
        "    (\"n_features\", jepa_cfg.get(\"n_features\")),\n",
        "    (\"n_time_features\", jepa_cfg.get(\"n_time_features\")),\n",
        "    (\"nhead\", jepa_cfg.get(\"nhead\")),\n",
        "    (\"num_layers\", jepa_cfg.get(\"num_layers\")),\n",
        "    (\"dim_ff\", jepa_cfg.get(\"dim_ff\")),\n",
        "    (\"dropout\", jepa_cfg.get(\"dropout\")),\n",
        "    (\"add_cls\", jepa_cfg.get(\"add_cls\")),\n",
        "    (\"pooling\", jepa_cfg.get(\"pooling\")),\n",
        "    (\"pred_len\", jepa_cfg.get(\"pred_len\")),\n",
        "    (\"ema_tau_min\", jepa_cfg.get(\"ema_tau_min\")),\n",
        "    (\"ema_tau_max\", jepa_cfg.get(\"ema_tau_max\")),\n",
        "    (\"use_asset_embeddings\", use_asset_embeddings),\n",
        "    (\"train_epochs\", train_cfg.get(\"epochs\")),\n",
        "    (\"train_batch_size\", train_cfg.get(\"batch_size_train\")),\n",
        "    (\"learning_rate\", train_cfg.get(\"learning_rate\")),\n",
        "    (\"dataset_root_path\", dataset_cfg.get(\"root_path\")),\n",
        "    (\"dataset_data_path\", dataset_cfg.get(\"data_path\")),\n",
        "    (\"dataset_timeframe\", dataset_cfg.get(\"timeframe\")),\n",
        "    (\"loss_type\", loss_cfg.get(\"loss_type\")),\n",
        "]\n",
        "\n",
        "hyper_df = pd.DataFrame(hyper_rows, columns=[\"key\", \"value\"])\n",
        "hyper_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_info = {\n",
        "    \"checkpoint_exists\": False,\n",
        "    \"checkpoint_path\": None,\n",
        "    \"checkpoint_epoch\": None,\n",
        "    \"checkpoint_monitor\": None,\n",
        "    \"asset_universe_size\": None,\n",
        "    \"state_dict_keys\": None,\n",
        "    \"missing_keys\": None,\n",
        "    \"unexpected_keys\": None,\n",
        "}\n",
        "\n",
        "if ckpt_path is not None and ckpt_path.exists():\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    state = ckpt.get(\"model\", ckpt.get(\"model_state_dict\", ckpt))\n",
        "\n",
        "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
        "\n",
        "    asset_universe = ckpt.get(\"asset_universe\")\n",
        "    checkpoint_info.update({\n",
        "        \"checkpoint_exists\": True,\n",
        "        \"checkpoint_path\": str(ckpt_path),\n",
        "        \"checkpoint_epoch\": ckpt.get(\"epoch\"),\n",
        "        \"checkpoint_monitor\": ckpt.get(\"monitor\"),\n",
        "        \"asset_universe_size\": len(asset_universe) if asset_universe is not None else None,\n",
        "        \"state_dict_keys\": len(state.keys()) if isinstance(state, dict) else None,\n",
        "        \"missing_keys\": len(missing),\n",
        "        \"unexpected_keys\": len(unexpected),\n",
        "    })\n",
        "\n",
        "    if SHOW_STATE_KEYS and isinstance(state, dict):\n",
        "        print(\"First state_dict keys:\")\n",
        "        for k in list(state.keys())[:STATE_KEYS_N]:\n",
        "            print(\" -\", k)\n",
        "\n",
        "checkpoint_df = pd.DataFrame(list(checkpoint_info.items()), columns=[\"key\", \"value\"])\n",
        "checkpoint_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Useful quick checks\n",
        "asset_emb_present = hasattr(model.context_enc, \"asset_emb\") and model.context_enc.asset_emb is not None\n",
        "print(f\"Context encoder has asset embedding module: {asset_emb_present}\")\n",
        "\n",
        "if ckpt_path is not None and ckpt_path.exists():\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    state = ckpt.get(\"model\", ckpt.get(\"model_state_dict\", ckpt))\n",
        "    asset_related = [k for k in state.keys() if \"asset_emb\" in k or \"asset_gate\" in k]\n",
        "    print(f\"Asset-related checkpoint tensors: {len(asset_related)}\")\n",
        "    if asset_related:\n",
        "        for k in asset_related[:20]:\n",
        "            t = state[k]\n",
        "            shape = tuple(t.shape) if hasattr(t, \"shape\") else None\n",
        "            print(f\" - {k}: {shape}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".graduenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
