{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO Model Testing\n",
        "\n",
        "Evaluate a trained PPO+JEPA model using a single training config file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: C:\\python\\koulu\\Gradu\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Resolve project root robustly when notebook is launched from different cwd\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    p = start.resolve()\n",
        "    for candidate in [p, *p.parents]:\n",
        "        if (candidate / \"src\").exists() and (candidate / \"configs\").exists():\n",
        "            return candidate\n",
        "    raise RuntimeError(\"Could not locate project root containing src/ and configs/\")\n",
        "\n",
        "PROJECT_ROOT = find_project_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "if str(PROJECT_ROOT / \"src\") not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
        "\n",
        "from config.config_utils import load_json_config\n",
        "from Datasets.multi_asset_dataset import Dataset_Finance_MultiAsset\n",
        "from Training.sb3_jepa_ppo import JEPAAuxFeatureExtractor, PPOWithJEPA\n",
        "from models.jepa.jepa import JEPA\n",
        "from models.time_series.patchTransformer import PatchTSTEncoder\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# User parameters\n",
        "# -----------------------------\n",
        "# Config used for training this PPO model\n",
        "PPO_CONFIG_PATH = \"configs/ppo_jepa_train1.json\"\n",
        "\n",
        "# Optional overrides (set to None to auto-resolve from config)\n",
        "PPO_CHECKPOINT_PATH = None\n",
        "JEPA_CHECKPOINT_PATH = \"checkpoints/jepa6_ppo1/jepa_step_700000.pt\"\n",
        "\n",
        "# If None, evaluate all assets available in validation split\n",
        "MAX_ASSETS = None\n",
        "\n",
        "# Deterministic policy during evaluation\n",
        "DETERMINISTIC = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model name: jepa6_ppo1\n",
            "PPO checkpoint: C:\\python\\koulu\\Gradu\\checkpoints\\jepa6_ppo1\\ppo_5600000_steps.zip\n",
            "JEPA checkpoint: C:\\python\\koulu\\Gradu\\checkpoints\\jepa6_ppo1\\jepa_step_700000.pt\n",
            "Action mode: discrete_3\n",
            "Asset universe: None\n"
          ]
        }
      ],
      "source": [
        "def get_latest_ppo_checkpoint(checkpoint_dir: str) -> str | None:\n",
        "    if not os.path.isdir(checkpoint_dir):\n",
        "        return None\n",
        "    ckpts = []\n",
        "    for fname in os.listdir(checkpoint_dir):\n",
        "        if fname.startswith(\"ppo_\") and fname.endswith(\"_steps.zip\"):\n",
        "            ckpts.append(os.path.join(checkpoint_dir, fname))\n",
        "    if not ckpts:\n",
        "        return None\n",
        "    ckpts.sort(key=lambda p: os.path.getmtime(p))\n",
        "    return ckpts[-1]\n",
        "\n",
        "\n",
        "def load_tickers(path: str) -> list | None:\n",
        "    if not path or not os.path.exists(path):\n",
        "        return None\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        tickers = [line.strip() for line in f if line.strip()]\n",
        "    return tickers or None\n",
        "\n",
        "\n",
        "def load_asset_universe_from_checkpoint(path: str | None) -> list | None:\n",
        "    if not path or not os.path.exists(path):\n",
        "        return None\n",
        "    try:\n",
        "        checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "    except Exception:\n",
        "        return None\n",
        "    asset_universe = checkpoint.get(\"asset_universe\")\n",
        "    return list(asset_universe) if asset_universe else None\n",
        "\n",
        "\n",
        "cfg = load_json_config(str(PROJECT_ROOT / PPO_CONFIG_PATH), \"\", str(PROJECT_ROOT / \"notebooks\" / \"ppo_model_test.ipynb\"))\n",
        "\n",
        "model_name = cfg[\"model_name\"]\n",
        "paths_cfg = cfg[\"paths\"]\n",
        "dataset_cfg = cfg[\"dataset\"]\n",
        "env_cfg = cfg[\"env\"]\n",
        "ppo_cfg = cfg[\"ppo\"]\n",
        "jepa_cfg = cfg[\"jepa_model\"]\n",
        "\n",
        "checkpoint_root = paths_cfg.get(\"checkpoint_root\", \"checkpoints\")\n",
        "log_root = paths_cfg.get(\"log_root\", \"logs\")\n",
        "ppo_checkpoint_dir = str(PROJECT_ROOT / checkpoint_root / model_name)\n",
        "\n",
        "if PPO_CHECKPOINT_PATH is None:\n",
        "    PPO_CHECKPOINT_PATH = get_latest_ppo_checkpoint(ppo_checkpoint_dir)\n",
        "if PPO_CHECKPOINT_PATH is None:\n",
        "    raise FileNotFoundError(f\"No PPO checkpoint found under {ppo_checkpoint_dir}\")\n",
        "if not os.path.isabs(PPO_CHECKPOINT_PATH):\n",
        "    PPO_CHECKPOINT_PATH = str(PROJECT_ROOT / PPO_CHECKPOINT_PATH)\n",
        "\n",
        "if JEPA_CHECKPOINT_PATH is None:\n",
        "    jepa_checkpoint_dir = paths_cfg[\"jepa_checkpoint_dir\"]\n",
        "    JEPA_CHECKPOINT_PATH = str(PROJECT_ROOT / jepa_checkpoint_dir / \"best.pt\")\n",
        "if not os.path.isabs(JEPA_CHECKPOINT_PATH):\n",
        "    JEPA_CHECKPOINT_PATH = str(PROJECT_ROOT / JEPA_CHECKPOINT_PATH)\n",
        "\n",
        "ACTION_MODE = env_cfg.get(\"action_mode\", \"continuous\")\n",
        "ALLOW_SHORT = env_cfg.get(\"allow_short\", True)\n",
        "INCLUDE_WEALTH = env_cfg.get(\"include_wealth\", True)\n",
        "TRANSACTION_COST = env_cfg[\"transaction_cost\"]\n",
        "\n",
        "print(\"Model name:\", model_name)\n",
        "print(\"PPO checkpoint:\", PPO_CHECKPOINT_PATH)\n",
        "print(\"JEPA checkpoint:\", JEPA_CHECKPOINT_PATH)\n",
        "print(\"Action mode:\", ACTION_MODE)\n",
        "print(\"Asset universe:\", paths_cfg[\"asset_universe_path\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class EvalConfig:\n",
        "    annual_trading_days: int = 252\n",
        "    regular_hours_only: bool = True\n",
        "    timeframe: str = \"15min\"\n",
        "    flat_threshold: float = 1e-3\n",
        "\n",
        "\n",
        "def _timeframe_to_minutes(timeframe: str) -> int:\n",
        "    tf = timeframe.strip().lower()\n",
        "    if tf.endswith(\"min\"):\n",
        "        return int(tf[:-3])\n",
        "    if tf.endswith(\"h\"):\n",
        "        return int(tf[:-1]) * 60\n",
        "    raise ValueError(f\"Unsupported timeframe: {timeframe}\")\n",
        "\n",
        "\n",
        "def annualization_factor(cfg: EvalConfig) -> float:\n",
        "    minutes_per_day = 390 if cfg.regular_hours_only else 24 * 60\n",
        "    minutes = _timeframe_to_minutes(cfg.timeframe)\n",
        "    bars_per_day = max(1, minutes_per_day // minutes)\n",
        "    return bars_per_day * cfg.annual_trading_days\n",
        "\n",
        "\n",
        "def action_to_weight(action) -> float:\n",
        "    if ACTION_MODE == \"discrete_3\":\n",
        "        discrete_actions = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n",
        "        idx = int(np.asarray(action).reshape(-1)[0])\n",
        "        idx = int(np.clip(idx, 0, len(discrete_actions) - 1))\n",
        "        w_t = float(discrete_actions[idx])\n",
        "    else:\n",
        "        w_t = float(np.clip(np.asarray(action).reshape(-1)[0], -1.0, 1.0))\n",
        "    if not ALLOW_SHORT:\n",
        "        w_t = max(0.0, w_t)\n",
        "    return w_t\n",
        "\n",
        "\n",
        "def compute_drawdown(equity: np.ndarray) -> float:\n",
        "    if equity.size == 0:\n",
        "        return float(\"nan\")\n",
        "    peak = np.maximum.accumulate(equity)\n",
        "    drawdown = (equity - peak) / peak\n",
        "    return float(np.min(drawdown))\n",
        "\n",
        "\n",
        "def safe_sharpe(mean: float, std: float, ann_factor: float) -> float:\n",
        "    if std <= 0 or np.isnan(std):\n",
        "        return float(\"nan\")\n",
        "    return float(mean / std * np.sqrt(ann_factor))\n",
        "\n",
        "\n",
        "def build_jepa_model(device: str, num_assets: int) -> JEPA:\n",
        "    encoder_num_assets = num_assets if jepa_cfg.get(\"use_asset_embeddings\", True) else None\n",
        "\n",
        "    jepa_context_encoder = PatchTSTEncoder(\n",
        "        patch_len=jepa_cfg[\"patch_len\"],\n",
        "        d_model=jepa_cfg[\"d_model\"],\n",
        "        n_features=jepa_cfg[\"n_features\"],\n",
        "        n_time_features=jepa_cfg[\"n_time_features\"],\n",
        "        nhead=jepa_cfg[\"nhead\"],\n",
        "        num_layers=jepa_cfg[\"num_layers\"],\n",
        "        dim_ff=jepa_cfg[\"dim_ff\"],\n",
        "        dropout=jepa_cfg[\"dropout\"],\n",
        "        add_cls=jepa_cfg.get(\"add_cls\", True),\n",
        "        pooling=jepa_cfg[\"pooling\"],\n",
        "        pred_len=jepa_cfg[\"pred_len\"],\n",
        "        num_assets=encoder_num_assets,\n",
        "    )\n",
        "    jepa_target_encoder = copy.deepcopy(jepa_context_encoder)\n",
        "\n",
        "    jepa_model = JEPA(\n",
        "        jepa_context_encoder,\n",
        "        jepa_target_encoder,\n",
        "        d_model=jepa_cfg[\"d_model\"],\n",
        "        ema_tau_min=jepa_cfg[\"ema_tau_min\"],\n",
        "        ema_tau_max=jepa_cfg[\"ema_tau_max\"],\n",
        "    )\n",
        "\n",
        "    if os.path.exists(JEPA_CHECKPOINT_PATH):\n",
        "        checkpoint = torch.load(JEPA_CHECKPOINT_PATH, map_location=\"cpu\")\n",
        "        missing, unexpected = jepa_model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
        "        if missing:\n",
        "            print(f\"Missing keys in JEPA checkpoint: {missing}\")\n",
        "        if unexpected:\n",
        "            print(f\"Unexpected keys in JEPA checkpoint: {unexpected}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"JEPA checkpoint not found: {JEPA_CHECKPOINT_PATH}\")\n",
        "\n",
        "    for param in jepa_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    jepa_model.eval()\n",
        "    return jepa_model.to(device)\n",
        "\n",
        "\n",
        "def load_ppo_model(model_path: str, device: str, policy_kwargs: Dict) -> PPOWithJEPA:\n",
        "    try:\n",
        "        return PPOWithJEPA.load(model_path, device=device)\n",
        "    except Exception as exc:\n",
        "        print(f\"Primary PPO load failed ({exc}); retrying with custom policy_kwargs.\")\n",
        "        return PPOWithJEPA.load(model_path, device=device, custom_objects={\"policy_kwargs\": policy_kwargs})\n",
        "\n",
        "\n",
        "def eval_asset(model: PPOWithJEPA, dataset: Dataset_Finance_MultiAsset, asset_id: str, cfg: EvalConfig) -> Dict[str, float]:\n",
        "    asset_idx = dataset.asset_id_to_idx.get(asset_id, -1)\n",
        "    X = dataset.data_x[asset_id]\n",
        "    dates = dataset.dates[asset_id]\n",
        "    ohlcv = dataset.ohlcv[asset_id]\n",
        "\n",
        "    seq_len = dataset.seq_len\n",
        "    pred_len = dataset.pred_len\n",
        "    n_steps = len(X) - seq_len - pred_len\n",
        "    if n_steps <= 0:\n",
        "        return {}\n",
        "\n",
        "    w_prev = 0.0\n",
        "    wealth = 1.0\n",
        "    rewards, asset_returns, positions, turnovers, equity = [], [], [], [], []\n",
        "\n",
        "    for cursor in range(n_steps):\n",
        "        x_context = X[cursor : cursor + seq_len].astype(np.float32)\n",
        "        t_context = dates[cursor : cursor + seq_len].astype(np.float32)\n",
        "        x_target = X[cursor + seq_len : cursor + seq_len + pred_len].astype(np.float32)\n",
        "        t_target = dates[cursor + seq_len : cursor + seq_len + pred_len].astype(np.float32)\n",
        "\n",
        "        obs = {\n",
        "            \"x_context\": x_context,\n",
        "            \"t_context\": t_context,\n",
        "            \"x_target\": x_target,\n",
        "            \"t_target\": t_target,\n",
        "            \"asset_id\": np.int64(asset_idx),\n",
        "            \"w_prev\": np.array([w_prev], dtype=np.float32),\n",
        "        }\n",
        "        if INCLUDE_WEALTH:\n",
        "            obs[\"wealth_feats\"] = np.array([np.log(wealth)], dtype=np.float32)\n",
        "\n",
        "        action, _ = model.predict(obs, deterministic=DETERMINISTIC)\n",
        "        w_t = action_to_weight(action)\n",
        "\n",
        "        close_t = float(ohlcv[cursor + seq_len - 1][3])\n",
        "        close_tp1 = float(ohlcv[cursor + seq_len][3])\n",
        "        r_tp1 = float(np.log(close_tp1 / close_t))\n",
        "\n",
        "        turnover = abs(w_t - w_prev)\n",
        "        reward = w_t * r_tp1 - TRANSACTION_COST * turnover\n",
        "        wealth *= float(np.exp(reward))\n",
        "\n",
        "        rewards.append(reward)\n",
        "        asset_returns.append(r_tp1)\n",
        "        positions.append(w_t)\n",
        "        turnovers.append(turnover)\n",
        "        equity.append(wealth)\n",
        "        w_prev = w_t\n",
        "\n",
        "    rewards = np.asarray(rewards, dtype=np.float64)\n",
        "    asset_returns = np.asarray(asset_returns, dtype=np.float64)\n",
        "    positions = np.asarray(positions, dtype=np.float64)\n",
        "    turnovers = np.asarray(turnovers, dtype=np.float64)\n",
        "    equity = np.asarray(equity, dtype=np.float64)\n",
        "\n",
        "    ann_factor = annualization_factor(cfg)\n",
        "    mean_reward = float(np.mean(rewards)) if rewards.size else float(\"nan\")\n",
        "    std_reward = float(np.std(rewards, ddof=1)) if rewards.size > 1 else float(\"nan\")\n",
        "\n",
        "    total_log_return = float(np.sum(rewards)) if rewards.size else float(\"nan\")\n",
        "    total_return = float(np.exp(total_log_return) - 1.0) if rewards.size else float(\"nan\")\n",
        "    annualized_return = float(np.exp(mean_reward * ann_factor) - 1.0) if rewards.size else float(\"nan\")\n",
        "    annualized_vol = float(std_reward * np.sqrt(ann_factor)) if rewards.size > 1 else float(\"nan\")\n",
        "    sharpe = safe_sharpe(mean_reward, std_reward, ann_factor)\n",
        "\n",
        "    downside = rewards[rewards < 0]\n",
        "    downside_std = float(np.std(downside, ddof=1)) if downside.size > 1 else float(\"nan\")\n",
        "    sortino = safe_sharpe(mean_reward, downside_std, ann_factor)\n",
        "\n",
        "    max_drawdown = compute_drawdown(equity)\n",
        "    calmar = float(annualized_return / abs(max_drawdown)) if max_drawdown < 0 else float(\"nan\")\n",
        "\n",
        "    win_rate = float(np.mean(rewards > 0)) if rewards.size else float(\"nan\")\n",
        "    avg_turnover = float(np.mean(turnovers)) if turnovers.size else float(\"nan\")\n",
        "    total_turnover = float(np.sum(turnovers)) if turnovers.size else float(\"nan\")\n",
        "    avg_position = float(np.mean(positions)) if positions.size else float(\"nan\")\n",
        "    pos_std = float(np.std(positions, ddof=1)) if positions.size > 1 else float(\"nan\")\n",
        "    abs_pos = float(np.mean(np.abs(positions))) if positions.size else float(\"nan\")\n",
        "\n",
        "    flat_mask = np.abs(positions) <= cfg.flat_threshold\n",
        "    long_mask = positions > cfg.flat_threshold\n",
        "    short_mask = positions < -cfg.flat_threshold\n",
        "    flat_frac = float(np.mean(flat_mask)) if positions.size else float(\"nan\")\n",
        "    long_frac = float(np.mean(long_mask)) if positions.size else float(\"nan\")\n",
        "    short_frac = float(np.mean(short_mask)) if positions.size else float(\"nan\")\n",
        "\n",
        "    trade_count = int(np.sum(np.abs(np.diff(positions)) > cfg.flat_threshold)) if positions.size > 1 else 0\n",
        "\n",
        "    bh_mean = float(np.mean(asset_returns)) if asset_returns.size else float(\"nan\")\n",
        "    bh_std = float(np.std(asset_returns, ddof=1)) if asset_returns.size > 1 else float(\"nan\")\n",
        "    bh_total_return = float(np.exp(np.sum(asset_returns)) - 1.0) if asset_returns.size else float(\"nan\")\n",
        "    bh_annualized_return = float(np.exp(bh_mean * ann_factor) - 1.0) if asset_returns.size else float(\"nan\")\n",
        "    bh_annualized_vol = float(bh_std * np.sqrt(ann_factor)) if asset_returns.size > 1 else float(\"nan\")\n",
        "    bh_sharpe = safe_sharpe(bh_mean, bh_std, ann_factor)\n",
        "\n",
        "    return {\n",
        "        \"asset_id\": asset_id,\n",
        "        \"steps\": int(n_steps),\n",
        "        \"total_return\": total_return,\n",
        "        \"annualized_return\": annualized_return,\n",
        "        \"annualized_volatility\": annualized_vol,\n",
        "        \"sharpe\": sharpe,\n",
        "        \"sortino\": sortino,\n",
        "        \"max_drawdown\": max_drawdown,\n",
        "        \"calmar\": calmar,\n",
        "        \"avg_reward\": mean_reward,\n",
        "        \"reward_volatility\": std_reward,\n",
        "        \"win_rate\": win_rate,\n",
        "        \"avg_turnover\": avg_turnover,\n",
        "        \"total_turnover\": total_turnover,\n",
        "        \"avg_position\": avg_position,\n",
        "        \"position_std\": pos_std,\n",
        "        \"avg_abs_position\": abs_pos,\n",
        "        \"long_frac\": long_frac,\n",
        "        \"short_frac\": short_frac,\n",
        "        \"flat_frac\": flat_frac,\n",
        "        \"trade_count\": trade_count,\n",
        "        \"bh_total_return\": bh_total_return,\n",
        "        \"bh_annualized_return\": bh_annualized_return,\n",
        "        \"bh_annualized_volatility\": bh_annualized_vol,\n",
        "        \"bh_sharpe\": bh_sharpe,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ollik\\AppData\\Local\\Temp\\ipykernel_51592\\3032108302.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path, map_location=\"cpu\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10 tickers from C:\\python\\koulu\\Gradu\\configs\\assets\\tickers0.txt\n",
            "tickers: ['CSCO', 'MRK', 'NKE', 'NVDA', 'DIS', 'AAPL', 'CAT', 'V', 'HON', 'AMZN']\n",
            "Loading evaluation dataset...\n",
            "[Dataset_Finance_MultiAsset] Global date splits: train_end=2024-08-01 15:15:00+00:00 val_end=2025-04-29 16:30:00+00:00 n_dates=33179 n_train=23225 n_val=4978 n_test=4976\n",
            "Assets to evaluate: 10\n"
          ]
        }
      ],
      "source": [
        "def _resolve_project_path(path_value: str | None) -> str | None:\n",
        "    if path_value is None:\n",
        "        return None\n",
        "    p = Path(path_value)\n",
        "    if p.is_absolute():\n",
        "        return str(p)\n",
        "    return str((PROJECT_ROOT / p).resolve())\n",
        "\n",
        "\n",
        "\n",
        "ticker_list_path = paths_cfg.get(\"ticker_list_path\")\n",
        "if not ticker_list_path:\n",
        "    raise ValueError(\"Config is missing paths.ticker_list_path\")\n",
        "\n",
        "# Build dataset from the same config used in training\n",
        "tickers_path = PROJECT_ROOT / ticker_list_path\n",
        "tickers = load_tickers(str(tickers_path))\n",
        "if not tickers:\n",
        "    raise RuntimeError(f\"No tickers loaded from {tickers_path}\")\n",
        "\n",
        "dataset_kwargs = {\n",
        "    \"root_path\": _resolve_project_path(dataset_cfg[\"root_path\"]),\n",
        "    \"data_path\": dataset_cfg[\"data_path\"],\n",
        "    \"start_date\": dataset_cfg.get(\"start_date\"),\n",
        "    \"split\": \"val\",\n",
        "    \"size\": [dataset_cfg[\"context_len\"], dataset_cfg[\"target_len\"]],\n",
        "    \"use_time_features\": dataset_cfg.get(\"use_time_features\", True),\n",
        "    \"rolling_window\": dataset_cfg[\"rolling_window\"],\n",
        "    \"train_split\": dataset_cfg[\"train_split\"],\n",
        "    \"test_split\": dataset_cfg[\"test_split\"],\n",
        "    \"regular_hours_only\": dataset_cfg.get(\"regular_hours_only\", True),\n",
        "    \"timeframe\": dataset_cfg.get(\"timeframe\", \"15min\"),\n",
        "    \"tickers\": tickers,\n",
        "}\n",
        "\n",
        "asset_universe = load_asset_universe_from_checkpoint(JEPA_CHECKPOINT_PATH)\n",
        "if asset_universe:\n",
        "    dataset_kwargs[\"asset_universe\"] = asset_universe\n",
        "\n",
        "dataset_kwargs[\"tickers\"] = tickers\n",
        "print(f\"Loaded {len(tickers)} tickers from {tickers_path}\")\n",
        "print(f\"tickers: {tickers}\")\n",
        "\n",
        "print(\"Loading evaluation dataset...\")\n",
        "test_dataset = Dataset_Finance_MultiAsset(**dataset_kwargs)\n",
        "if not test_dataset.asset_ids:\n",
        "    raise RuntimeError(\"No assets found in the validation dataset.\")\n",
        "\n",
        "if MAX_ASSETS is not None:\n",
        "    test_dataset.asset_ids = test_dataset.asset_ids[: int(MAX_ASSETS)]\n",
        "\n",
        "print(f\"Assets to evaluate: {len(test_dataset.asset_ids)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading JEPA model...\n",
            "Loading PPO model from C:\\python\\koulu\\Gradu\\checkpoints\\jepa6_ppo1\\ppo_5600000_steps.zip...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ollik\\miniconda3\\envs\\.graduenv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "C:\\Users\\ollik\\AppData\\Local\\Temp\\ipykernel_51592\\3818601127.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(JEPA_CHECKPOINT_PATH, map_location=\"cpu\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MultiInputActorCriticPolicy(\n",
              "  (features_extractor): JEPAAuxFeatureExtractor(\n",
              "    (jepa_model): JEPA(\n",
              "      (context_enc): PatchTSTEncoder(\n",
              "        (proj_price): Linear(in_features=72, out_features=192, bias=True)\n",
              "        (proj_time): Linear(in_features=32, out_features=192, bias=True)\n",
              "        (posenc): PositionalEncoding()\n",
              "        (encoder): TransformerEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-3): 4 x TransformerEncoderLayer(\n",
              "              (self_attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "              )\n",
              "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
              "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout1): Dropout(p=0.0, inplace=False)\n",
              "              (dropout2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "        (head): Identity()\n",
              "      )\n",
              "      (target_enc): PatchTSTEncoder(\n",
              "        (proj_price): Linear(in_features=72, out_features=192, bias=True)\n",
              "        (proj_time): Linear(in_features=32, out_features=192, bias=True)\n",
              "        (posenc): PositionalEncoding()\n",
              "        (encoder): TransformerEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-3): 4 x TransformerEncoderLayer(\n",
              "              (self_attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "              )\n",
              "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
              "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout1): Dropout(p=0.0, inplace=False)\n",
              "              (dropout2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "        (head): Identity()\n",
              "      )\n",
              "      (predictor): Sequential(\n",
              "        (0): Linear(in_features=192, out_features=384, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (3): GELU(approximate='none')\n",
              "        (4): Linear(in_features=384, out_features=192, bias=True)\n",
              "        (5): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pi_features_extractor): JEPAAuxFeatureExtractor(\n",
              "    (jepa_model): JEPA(\n",
              "      (context_enc): PatchTSTEncoder(\n",
              "        (proj_price): Linear(in_features=72, out_features=192, bias=True)\n",
              "        (proj_time): Linear(in_features=32, out_features=192, bias=True)\n",
              "        (posenc): PositionalEncoding()\n",
              "        (encoder): TransformerEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-3): 4 x TransformerEncoderLayer(\n",
              "              (self_attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "              )\n",
              "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
              "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout1): Dropout(p=0.0, inplace=False)\n",
              "              (dropout2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "        (head): Identity()\n",
              "      )\n",
              "      (target_enc): PatchTSTEncoder(\n",
              "        (proj_price): Linear(in_features=72, out_features=192, bias=True)\n",
              "        (proj_time): Linear(in_features=32, out_features=192, bias=True)\n",
              "        (posenc): PositionalEncoding()\n",
              "        (encoder): TransformerEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-3): 4 x TransformerEncoderLayer(\n",
              "              (self_attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "              )\n",
              "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
              "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout1): Dropout(p=0.0, inplace=False)\n",
              "              (dropout2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "        (head): Identity()\n",
              "      )\n",
              "      (predictor): Sequential(\n",
              "        (0): Linear(in_features=192, out_features=384, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (3): GELU(approximate='none')\n",
              "        (4): Linear(in_features=384, out_features=192, bias=True)\n",
              "        (5): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (vf_features_extractor): JEPAAuxFeatureExtractor(\n",
              "    (jepa_model): JEPA(\n",
              "      (context_enc): PatchTSTEncoder(\n",
              "        (proj_price): Linear(in_features=72, out_features=192, bias=True)\n",
              "        (proj_time): Linear(in_features=32, out_features=192, bias=True)\n",
              "        (posenc): PositionalEncoding()\n",
              "        (encoder): TransformerEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-3): 4 x TransformerEncoderLayer(\n",
              "              (self_attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "              )\n",
              "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
              "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout1): Dropout(p=0.0, inplace=False)\n",
              "              (dropout2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "        (head): Identity()\n",
              "      )\n",
              "      (target_enc): PatchTSTEncoder(\n",
              "        (proj_price): Linear(in_features=72, out_features=192, bias=True)\n",
              "        (proj_time): Linear(in_features=32, out_features=192, bias=True)\n",
              "        (posenc): PositionalEncoding()\n",
              "        (encoder): TransformerEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-3): 4 x TransformerEncoderLayer(\n",
              "              (self_attn): MultiheadAttention(\n",
              "                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "              )\n",
              "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
              "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout1): Dropout(p=0.0, inplace=False)\n",
              "              (dropout2): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "        (head): Identity()\n",
              "      )\n",
              "      (predictor): Sequential(\n",
              "        (0): Linear(in_features=192, out_features=384, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (3): GELU(approximate='none')\n",
              "        (4): Linear(in_features=384, out_features=192, bias=True)\n",
              "        (5): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlp_extractor): MlpExtractor(\n",
              "    (policy_net): Sequential(\n",
              "      (0): Linear(in_features=227, out_features=256, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): Tanh()\n",
              "    )\n",
              "    (value_net): Sequential(\n",
              "      (0): Linear(in_features=227, out_features=256, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (action_net): Linear(in_features=256, out_features=3, bias=True)\n",
              "  (value_net): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_asset_ids = int(getattr(test_dataset, \"num_asset_ids\", len(test_dataset.asset_ids)))\n",
        "\n",
        "print(\"Loading JEPA model...\")\n",
        "jepa_model = build_jepa_model(device, num_assets=num_asset_ids)\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=JEPAAuxFeatureExtractor,\n",
        "    features_extractor_kwargs=dict(\n",
        "        jepa_model=jepa_model,\n",
        "        embedding_dim=jepa_cfg[\"d_model\"],\n",
        "        patch_len=jepa_cfg[\"patch_len\"],\n",
        "        patch_stride=jepa_cfg[\"patch_stride\"],\n",
        "        use_obs_targets=True,\n",
        "        target_len=test_dataset.pred_len,\n",
        "    ),\n",
        "    net_arch=dict(pi=[256, 256], vf=[256, 256]),\n",
        ")\n",
        "\n",
        "print(f\"Loading PPO model from {PPO_CHECKPOINT_PATH}...\")\n",
        "model = load_ppo_model(PPO_CHECKPOINT_PATH, device=device, policy_kwargs=policy_kwargs)\n",
        "model.policy.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/10] Evaluating CSCO...\n",
            "[2/10] Evaluating MRK...\n",
            "[3/10] Evaluating NKE...\n",
            "[4/10] Evaluating NVDA...\n",
            "[5/10] Evaluating DIS...\n",
            "[6/10] Evaluating AAPL...\n",
            "[7/10] Evaluating CAT...\n",
            "[8/10] Evaluating V...\n",
            "[9/10] Evaluating HON...\n",
            "[10/10] Evaluating AMZN...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>steps</th>\n",
              "      <th>total_return</th>\n",
              "      <th>annualized_return</th>\n",
              "      <th>annualized_volatility</th>\n",
              "      <th>sharpe</th>\n",
              "      <th>sortino</th>\n",
              "      <th>max_drawdown</th>\n",
              "      <th>calmar</th>\n",
              "      <th>avg_reward</th>\n",
              "      <th>...</th>\n",
              "      <th>position_std</th>\n",
              "      <th>avg_abs_position</th>\n",
              "      <th>long_frac</th>\n",
              "      <th>short_frac</th>\n",
              "      <th>flat_frac</th>\n",
              "      <th>trade_count</th>\n",
              "      <th>bh_total_return</th>\n",
              "      <th>bh_annualized_return</th>\n",
              "      <th>bh_annualized_volatility</th>\n",
              "      <th>bh_sharpe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>4417</td>\n",
              "      <td>-0.099811</td>\n",
              "      <td>-0.144421</td>\n",
              "      <td>0.224262</td>\n",
              "      <td>-0.695511</td>\n",
              "      <td>-0.506970</td>\n",
              "      <td>-0.236479</td>\n",
              "      <td>-0.610714</td>\n",
              "      <td>-0.000024</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497466</td>\n",
              "      <td>0.449174</td>\n",
              "      <td>0.449174</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.550826</td>\n",
              "      <td>1950</td>\n",
              "      <td>-0.081011</td>\n",
              "      <td>-0.117782</td>\n",
              "      <td>0.340704</td>\n",
              "      <td>-0.367817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>AMZN</td>\n",
              "      <td>4417</td>\n",
              "      <td>0.142042</td>\n",
              "      <td>0.217764</td>\n",
              "      <td>0.298332</td>\n",
              "      <td>0.660395</td>\n",
              "      <td>0.577595</td>\n",
              "      <td>-0.349401</td>\n",
              "      <td>0.623251</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497443</td>\n",
              "      <td>0.551053</td>\n",
              "      <td>0.551053</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.448947</td>\n",
              "      <td>1936</td>\n",
              "      <td>0.104821</td>\n",
              "      <td>0.159358</td>\n",
              "      <td>0.362985</td>\n",
              "      <td>0.407364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>CAT</td>\n",
              "      <td>4416</td>\n",
              "      <td>-0.147765</td>\n",
              "      <td>-0.211192</td>\n",
              "      <td>0.279243</td>\n",
              "      <td>-0.849554</td>\n",
              "      <td>-0.682170</td>\n",
              "      <td>-0.301900</td>\n",
              "      <td>-0.699542</td>\n",
              "      <td>-0.000036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.499162</td>\n",
              "      <td>0.529891</td>\n",
              "      <td>0.529891</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.470109</td>\n",
              "      <td>1856</td>\n",
              "      <td>-0.130198</td>\n",
              "      <td>-0.186948</td>\n",
              "      <td>0.354228</td>\n",
              "      <td>-0.584258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CSCO</td>\n",
              "      <td>4416</td>\n",
              "      <td>0.039338</td>\n",
              "      <td>0.058917</td>\n",
              "      <td>0.159065</td>\n",
              "      <td>0.359894</td>\n",
              "      <td>0.275801</td>\n",
              "      <td>-0.155421</td>\n",
              "      <td>0.379080</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>...</td>\n",
              "      <td>0.499390</td>\n",
              "      <td>0.474185</td>\n",
              "      <td>0.474185</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.525815</td>\n",
              "      <td>1862</td>\n",
              "      <td>0.130017</td>\n",
              "      <td>0.198842</td>\n",
              "      <td>0.235809</td>\n",
              "      <td>0.769079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DIS</td>\n",
              "      <td>4370</td>\n",
              "      <td>-0.085133</td>\n",
              "      <td>-0.124888</td>\n",
              "      <td>0.260089</td>\n",
              "      <td>-0.512916</td>\n",
              "      <td>-0.445220</td>\n",
              "      <td>-0.271276</td>\n",
              "      <td>-0.460374</td>\n",
              "      <td>-0.000020</td>\n",
              "      <td>...</td>\n",
              "      <td>0.499801</td>\n",
              "      <td>0.516018</td>\n",
              "      <td>0.516018</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.483982</td>\n",
              "      <td>1770</td>\n",
              "      <td>0.007318</td>\n",
              "      <td>0.010991</td>\n",
              "      <td>0.306695</td>\n",
              "      <td>0.035642</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  asset_id  steps  total_return  annualized_return  annualized_volatility  \\\n",
              "5     AAPL   4417     -0.099811          -0.144421               0.224262   \n",
              "9     AMZN   4417      0.142042           0.217764               0.298332   \n",
              "6      CAT   4416     -0.147765          -0.211192               0.279243   \n",
              "0     CSCO   4416      0.039338           0.058917               0.159065   \n",
              "4      DIS   4370     -0.085133          -0.124888               0.260089   \n",
              "\n",
              "     sharpe   sortino  max_drawdown    calmar  avg_reward  ...  position_std  \\\n",
              "5 -0.695511 -0.506970     -0.236479 -0.610714   -0.000024  ...      0.497466   \n",
              "9  0.660395  0.577595     -0.349401  0.623251    0.000030  ...      0.497443   \n",
              "6 -0.849554 -0.682170     -0.301900 -0.699542   -0.000036  ...      0.499162   \n",
              "0  0.359894  0.275801     -0.155421  0.379080    0.000009  ...      0.499390   \n",
              "4 -0.512916 -0.445220     -0.271276 -0.460374   -0.000020  ...      0.499801   \n",
              "\n",
              "   avg_abs_position  long_frac  short_frac  flat_frac  trade_count  \\\n",
              "5          0.449174   0.449174         0.0   0.550826         1950   \n",
              "9          0.551053   0.551053         0.0   0.448947         1936   \n",
              "6          0.529891   0.529891         0.0   0.470109         1856   \n",
              "0          0.474185   0.474185         0.0   0.525815         1862   \n",
              "4          0.516018   0.516018         0.0   0.483982         1770   \n",
              "\n",
              "   bh_total_return  bh_annualized_return  bh_annualized_volatility  bh_sharpe  \n",
              "5        -0.081011             -0.117782                  0.340704  -0.367817  \n",
              "9         0.104821              0.159358                  0.362985   0.407364  \n",
              "6        -0.130198             -0.186948                  0.354228  -0.584258  \n",
              "0         0.130017              0.198842                  0.235809   0.769079  \n",
              "4         0.007318              0.010991                  0.306695   0.035642  \n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_cfg = EvalConfig(\n",
        "    annual_trading_days=252,\n",
        "    regular_hours_only=dataset_kwargs.get(\"regular_hours_only\", True),\n",
        "    timeframe=dataset_kwargs.get(\"timeframe\", \"15min\"),\n",
        ")\n",
        "\n",
        "results: List[Dict[str, float]] = []\n",
        "for idx, asset_id in enumerate(test_dataset.asset_ids, start=1):\n",
        "    print(f\"[{idx}/{len(test_dataset.asset_ids)}] Evaluating {asset_id}...\")\n",
        "    metrics = eval_asset(model, test_dataset, asset_id, eval_cfg)\n",
        "    if metrics:\n",
        "        results.append(metrics)\n",
        "\n",
        "if not results:\n",
        "    raise RuntimeError(\"No evaluation results produced.\")\n",
        "\n",
        "df = pd.DataFrame(results).sort_values(\"asset_id\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved per-asset metrics to C:\\python\\koulu\\Gradu\\logs\\jepa6_ppo1_test_metrics.csv\n",
            "Saved summary to C:\\python\\koulu\\Gradu\\logs\\jepa6_ppo1_test_summary.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>steps</th>\n",
              "      <th>total_return</th>\n",
              "      <th>annualized_return</th>\n",
              "      <th>annualized_volatility</th>\n",
              "      <th>sharpe</th>\n",
              "      <th>sortino</th>\n",
              "      <th>max_drawdown</th>\n",
              "      <th>calmar</th>\n",
              "      <th>avg_reward</th>\n",
              "      <th>reward_volatility</th>\n",
              "      <th>...</th>\n",
              "      <th>position_std</th>\n",
              "      <th>avg_abs_position</th>\n",
              "      <th>long_frac</th>\n",
              "      <th>short_frac</th>\n",
              "      <th>flat_frac</th>\n",
              "      <th>trade_count</th>\n",
              "      <th>bh_total_return</th>\n",
              "      <th>bh_annualized_return</th>\n",
              "      <th>bh_annualized_volatility</th>\n",
              "      <th>bh_sharpe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4408.9</td>\n",
              "      <td>-0.024248</td>\n",
              "      <td>-0.028455</td>\n",
              "      <td>0.257393</td>\n",
              "      <td>-0.131577</td>\n",
              "      <td>-0.052832</td>\n",
              "      <td>-0.249422</td>\n",
              "      <td>0.216139</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>0.003180</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497378</td>\n",
              "      <td>0.522059</td>\n",
              "      <td>0.522059</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.477941</td>\n",
              "      <td>1849.5</td>\n",
              "      <td>-0.048963</td>\n",
              "      <td>-0.061322</td>\n",
              "      <td>0.334609</td>\n",
              "      <td>-0.212219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>median</th>\n",
              "      <td>4416.0</td>\n",
              "      <td>-0.039787</td>\n",
              "      <td>-0.058316</td>\n",
              "      <td>0.247066</td>\n",
              "      <td>-0.246423</td>\n",
              "      <td>-0.214347</td>\n",
              "      <td>-0.276630</td>\n",
              "      <td>-0.217825</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>0.003052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497929</td>\n",
              "      <td>0.528793</td>\n",
              "      <td>0.528793</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.471207</td>\n",
              "      <td>1856.0</td>\n",
              "      <td>-0.056866</td>\n",
              "      <td>-0.082971</td>\n",
              "      <td>0.323700</td>\n",
              "      <td>-0.251851</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows Ã— 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         steps  total_return  annualized_return  annualized_volatility  \\\n",
              "mean    4408.9     -0.024248          -0.028455               0.257393   \n",
              "median  4416.0     -0.039787          -0.058316               0.247066   \n",
              "\n",
              "          sharpe   sortino  max_drawdown    calmar  avg_reward  \\\n",
              "mean   -0.131577 -0.052832     -0.249422  0.216139   -0.000008   \n",
              "median -0.246423 -0.214347     -0.276630 -0.217825   -0.000010   \n",
              "\n",
              "        reward_volatility  ...  position_std  avg_abs_position  long_frac  \\\n",
              "mean             0.003180  ...      0.497378          0.522059   0.522059   \n",
              "median           0.003052  ...      0.497929          0.528793   0.528793   \n",
              "\n",
              "        short_frac  flat_frac  trade_count  bh_total_return  \\\n",
              "mean           0.0   0.477941       1849.5        -0.048963   \n",
              "median         0.0   0.471207       1856.0        -0.056866   \n",
              "\n",
              "        bh_annualized_return  bh_annualized_volatility  bh_sharpe  \n",
              "mean               -0.061322                  0.334609  -0.212219  \n",
              "median             -0.082971                  0.323700  -0.251851  \n",
              "\n",
              "[2 rows x 24 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save outputs\n",
        "os.makedirs(PROJECT_ROOT / log_root, exist_ok=True)\n",
        "metrics_path = PROJECT_ROOT / log_root / f\"{model_name}_test_metrics.csv\"\n",
        "summary_path = PROJECT_ROOT / log_root / f\"{model_name}_test_summary.csv\"\n",
        "\n",
        "df.to_csv(metrics_path, index=False)\n",
        "summary = df.drop(columns=[\"asset_id\"]).agg([\"mean\", \"median\"])\n",
        "summary.to_csv(summary_path)\n",
        "\n",
        "print(f\"Saved per-asset metrics to {metrics_path}\")\n",
        "print(f\"Saved summary to {summary_path}\")\n",
        "summary\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".graduenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
