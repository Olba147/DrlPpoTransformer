{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO Model Testing\n",
        "\n",
        "Evaluate a trained PPO+JEPA model using a single training config file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: C:\\python\\koulu\\Gradu\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Resolve project root robustly when notebook is launched from different cwd\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    p = start.resolve()\n",
        "    for candidate in [p, *p.parents]:\n",
        "        if (candidate / \"src\").exists() and (candidate / \"configs\").exists():\n",
        "            return candidate\n",
        "    raise RuntimeError(\"Could not locate project root containing src/ and configs/\")\n",
        "\n",
        "PROJECT_ROOT = find_project_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "if str(PROJECT_ROOT / \"src\") not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
        "\n",
        "from config.config_utils import load_json_config\n",
        "from Datasets.multi_asset_dataset import Dataset_Finance_MultiAsset\n",
        "from Training.sb3_jepa_ppo import JEPAAuxFeatureExtractor, PPOWithJEPA\n",
        "from models.jepa.jepa import JEPA\n",
        "from models.time_series.patchTransformer import PatchTSTEncoder\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# User parameters\n",
        "# -----------------------------\n",
        "# Config used for training this PPO model\n",
        "PPO_CONFIG_PATH = \"configs/ppo_jepa_train1.json\"\n",
        "\n",
        "# Optional overrides (set to None to auto-resolve from config)\n",
        "PPO_CHECKPOINT_PATH = None\n",
        "JEPA_CHECKPOINT_PATH = \"checkpoints/jepa6_ppo1/jepa_step_700000.pt\"\n",
        "\n",
        "# If None, evaluate all assets available in validation split\n",
        "MAX_ASSETS = None\n",
        "\n",
        "# Deterministic policy during evaluation\n",
        "DETERMINISTIC = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model name: jepa6_ppo1\n",
            "PPO checkpoint: C:\\python\\koulu\\Gradu\\checkpoints\\jepa6_ppo1\\ppo_5600000_steps.zip\n",
            "JEPA checkpoint: C:\\python\\koulu\\Gradu\\checkpoints\\jepa6_ppo1\\jepa_step_700000.pt\n",
            "Action mode: discrete_3\n"
          ]
        }
      ],
      "source": [
        "def get_latest_ppo_checkpoint(checkpoint_dir: str) -> str | None:\n",
        "    if not os.path.isdir(checkpoint_dir):\n",
        "        return None\n",
        "    ckpts = []\n",
        "    for fname in os.listdir(checkpoint_dir):\n",
        "        if fname.startswith(\"ppo_\") and fname.endswith(\"_steps.zip\"):\n",
        "            ckpts.append(os.path.join(checkpoint_dir, fname))\n",
        "    if not ckpts:\n",
        "        return None\n",
        "    ckpts.sort(key=lambda p: os.path.getmtime(p))\n",
        "    return ckpts[-1]\n",
        "\n",
        "\n",
        "def load_tickers(path: str) -> list | None:\n",
        "    if not path or not os.path.exists(path):\n",
        "        return None\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        tickers = [line.strip() for line in f if line.strip()]\n",
        "    return tickers or None\n",
        "\n",
        "\n",
        "def load_asset_universe_from_checkpoint(path: str | None) -> list | None:\n",
        "    if not path or not os.path.exists(path):\n",
        "        return None\n",
        "    try:\n",
        "        checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "    except Exception:\n",
        "        return None\n",
        "    asset_universe = checkpoint.get(\"asset_universe\")\n",
        "    return list(asset_universe) if asset_universe else None\n",
        "\n",
        "\n",
        "cfg = load_json_config(str(PROJECT_ROOT / PPO_CONFIG_PATH), \"\", str(PROJECT_ROOT / \"notebooks\" / \"ppo_model_test.ipynb\"))\n",
        "\n",
        "model_name = cfg[\"model_name\"]\n",
        "paths_cfg = cfg[\"paths\"]\n",
        "dataset_cfg = cfg[\"dataset\"]\n",
        "env_cfg = cfg[\"env\"]\n",
        "ppo_cfg = cfg[\"ppo\"]\n",
        "jepa_cfg = cfg[\"jepa_model\"]\n",
        "\n",
        "checkpoint_root = paths_cfg.get(\"checkpoint_root\", \"checkpoints\")\n",
        "log_root = paths_cfg.get(\"log_root\", \"logs\")\n",
        "ppo_checkpoint_dir = str(PROJECT_ROOT / checkpoint_root / model_name)\n",
        "\n",
        "if PPO_CHECKPOINT_PATH is None:\n",
        "    PPO_CHECKPOINT_PATH = get_latest_ppo_checkpoint(ppo_checkpoint_dir)\n",
        "if PPO_CHECKPOINT_PATH is None:\n",
        "    raise FileNotFoundError(f\"No PPO checkpoint found under {ppo_checkpoint_dir}\")\n",
        "if not os.path.isabs(PPO_CHECKPOINT_PATH):\n",
        "    PPO_CHECKPOINT_PATH = str(PROJECT_ROOT / PPO_CHECKPOINT_PATH)\n",
        "\n",
        "if JEPA_CHECKPOINT_PATH is None:\n",
        "    jepa_checkpoint_dir = paths_cfg[\"jepa_checkpoint_dir\"]\n",
        "    JEPA_CHECKPOINT_PATH = str(PROJECT_ROOT / jepa_checkpoint_dir / \"best.pt\")\n",
        "if not os.path.isabs(JEPA_CHECKPOINT_PATH):\n",
        "    JEPA_CHECKPOINT_PATH = str(PROJECT_ROOT / JEPA_CHECKPOINT_PATH)\n",
        "\n",
        "ACTION_MODE = env_cfg.get(\"action_mode\", \"continuous\")\n",
        "ALLOW_SHORT = env_cfg.get(\"allow_short\", True)\n",
        "INCLUDE_WEALTH = env_cfg.get(\"include_wealth\", True)\n",
        "TRANSACTION_COST = env_cfg[\"transaction_cost\"]\n",
        "\n",
        "print(\"Model name:\", model_name)\n",
        "print(\"PPO checkpoint:\", PPO_CHECKPOINT_PATH)\n",
        "print(\"JEPA checkpoint:\", JEPA_CHECKPOINT_PATH)\n",
        "print(\"Action mode:\", ACTION_MODE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class EvalConfig:\n",
        "    annual_trading_days: int = 252\n",
        "    regular_hours_only: bool = True\n",
        "    timeframe: str = \"15min\"\n",
        "    flat_threshold: float = 1e-3\n",
        "\n",
        "\n",
        "def _timeframe_to_minutes(timeframe: str) -> int:\n",
        "    tf = timeframe.strip().lower()\n",
        "    if tf.endswith(\"min\"):\n",
        "        return int(tf[:-3])\n",
        "    if tf.endswith(\"h\"):\n",
        "        return int(tf[:-1]) * 60\n",
        "    raise ValueError(f\"Unsupported timeframe: {timeframe}\")\n",
        "\n",
        "\n",
        "def annualization_factor(cfg: EvalConfig) -> float:\n",
        "    minutes_per_day = 390 if cfg.regular_hours_only else 24 * 60\n",
        "    minutes = _timeframe_to_minutes(cfg.timeframe)\n",
        "    bars_per_day = max(1, minutes_per_day // minutes)\n",
        "    return bars_per_day * cfg.annual_trading_days\n",
        "\n",
        "\n",
        "def action_to_weight(action) -> float:\n",
        "    if ACTION_MODE == \"discrete_3\":\n",
        "        discrete_actions = np.array([-1.0, 0.0, 1.0], dtype=np.float32)\n",
        "        idx = int(np.asarray(action).reshape(-1)[0])\n",
        "        idx = int(np.clip(idx, 0, len(discrete_actions) - 1))\n",
        "        w_t = float(discrete_actions[idx])\n",
        "    else:\n",
        "        w_t = float(np.clip(np.asarray(action).reshape(-1)[0], -1.0, 1.0))\n",
        "    if not ALLOW_SHORT:\n",
        "        w_t = max(0.0, w_t)\n",
        "    return w_t\n",
        "\n",
        "\n",
        "def compute_drawdown(equity: np.ndarray) -> float:\n",
        "    if equity.size == 0:\n",
        "        return float(\"nan\")\n",
        "    peak = np.maximum.accumulate(equity)\n",
        "    drawdown = (equity - peak) / peak\n",
        "    return float(np.min(drawdown))\n",
        "\n",
        "\n",
        "def safe_sharpe(mean: float, std: float, ann_factor: float) -> float:\n",
        "    if std <= 0 or np.isnan(std):\n",
        "        return float(\"nan\")\n",
        "    return float(mean / std * np.sqrt(ann_factor))\n",
        "\n",
        "\n",
        "def build_jepa_model(device: str, num_assets: int) -> JEPA:\n",
        "    encoder_num_assets = num_assets if jepa_cfg.get(\"use_asset_embeddings\", True) else None\n",
        "\n",
        "    jepa_context_encoder = PatchTSTEncoder(\n",
        "        patch_len=jepa_cfg[\"patch_len\"],\n",
        "        d_model=jepa_cfg[\"d_model\"],\n",
        "        n_features=jepa_cfg[\"n_features\"],\n",
        "        n_time_features=jepa_cfg[\"n_time_features\"],\n",
        "        nhead=jepa_cfg[\"nhead\"],\n",
        "        num_layers=jepa_cfg[\"num_layers\"],\n",
        "        dim_ff=jepa_cfg[\"dim_ff\"],\n",
        "        dropout=jepa_cfg[\"dropout\"],\n",
        "        add_cls=jepa_cfg.get(\"add_cls\", True),\n",
        "        pooling=jepa_cfg[\"pooling\"],\n",
        "        pred_len=jepa_cfg[\"pred_len\"],\n",
        "        num_assets=encoder_num_assets,\n",
        "    )\n",
        "    jepa_target_encoder = copy.deepcopy(jepa_context_encoder)\n",
        "\n",
        "    jepa_model = JEPA(\n",
        "        jepa_context_encoder,\n",
        "        jepa_target_encoder,\n",
        "        d_model=jepa_cfg[\"d_model\"],\n",
        "        ema_tau_min=jepa_cfg[\"ema_tau_min\"],\n",
        "        ema_tau_max=jepa_cfg[\"ema_tau_max\"],\n",
        "    )\n",
        "\n",
        "    if os.path.exists(JEPA_CHECKPOINT_PATH):\n",
        "        checkpoint = torch.load(JEPA_CHECKPOINT_PATH, map_location=\"cpu\")\n",
        "        missing, unexpected = jepa_model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
        "        if missing:\n",
        "            print(f\"Missing keys in JEPA checkpoint: {missing}\")\n",
        "        if unexpected:\n",
        "            print(f\"Unexpected keys in JEPA checkpoint: {unexpected}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"JEPA checkpoint not found: {JEPA_CHECKPOINT_PATH}\")\n",
        "\n",
        "    for param in jepa_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    jepa_model.eval()\n",
        "    return jepa_model.to(device)\n",
        "\n",
        "\n",
        "def load_ppo_model(model_path: str, device: str, policy_kwargs: Dict) -> PPOWithJEPA:\n",
        "    try:\n",
        "        return PPOWithJEPA.load(model_path, device=device)\n",
        "    except Exception as exc:\n",
        "        print(f\"Primary PPO load failed ({exc}); retrying with custom policy_kwargs.\")\n",
        "        return PPOWithJEPA.load(model_path, device=device, custom_objects={\"policy_kwargs\": policy_kwargs})\n",
        "\n",
        "\n",
        "def eval_asset(model: PPOWithJEPA, dataset: Dataset_Finance_MultiAsset, asset_id: str, cfg: EvalConfig) -> Dict[str, float]:\n",
        "    asset_idx = dataset.asset_id_to_idx.get(asset_id, -1)\n",
        "    X = dataset.data_x[asset_id]\n",
        "    dates = dataset.dates[asset_id]\n",
        "    ohlcv = dataset.ohlcv[asset_id]\n",
        "\n",
        "    seq_len = dataset.seq_len\n",
        "    pred_len = dataset.pred_len\n",
        "    n_steps = len(X) - seq_len - pred_len\n",
        "    if n_steps <= 0:\n",
        "        return {}\n",
        "\n",
        "    w_prev = 0.0\n",
        "    wealth = 1.0\n",
        "    rewards, asset_returns, positions, turnovers, equity = [], [], [], [], []\n",
        "\n",
        "    for cursor in range(n_steps):\n",
        "        x_context = X[cursor : cursor + seq_len].astype(np.float32)\n",
        "        t_context = dates[cursor : cursor + seq_len].astype(np.float32)\n",
        "        x_target = X[cursor + seq_len : cursor + seq_len + pred_len].astype(np.float32)\n",
        "        t_target = dates[cursor + seq_len : cursor + seq_len + pred_len].astype(np.float32)\n",
        "\n",
        "        obs = {\n",
        "            \"x_context\": x_context,\n",
        "            \"t_context\": t_context,\n",
        "            \"x_target\": x_target,\n",
        "            \"t_target\": t_target,\n",
        "            \"asset_id\": np.int64(asset_idx),\n",
        "            \"w_prev\": np.array([w_prev], dtype=np.float32),\n",
        "        }\n",
        "        if INCLUDE_WEALTH:\n",
        "            obs[\"wealth_feats\"] = np.array([np.log(wealth)], dtype=np.float32)\n",
        "\n",
        "        action, _ = model.predict(obs, deterministic=DETERMINISTIC)\n",
        "        w_t = action_to_weight(action)\n",
        "\n",
        "        close_t = float(ohlcv[cursor + seq_len - 1][3])\n",
        "        close_tp1 = float(ohlcv[cursor + seq_len][3])\n",
        "        r_tp1 = float(np.log(close_tp1 / close_t))\n",
        "\n",
        "        turnover = abs(w_t - w_prev)\n",
        "        reward = w_t * r_tp1 - TRANSACTION_COST * turnover\n",
        "        wealth *= float(np.exp(reward))\n",
        "\n",
        "        rewards.append(reward)\n",
        "        asset_returns.append(r_tp1)\n",
        "        positions.append(w_t)\n",
        "        turnovers.append(turnover)\n",
        "        equity.append(wealth)\n",
        "        w_prev = w_t\n",
        "\n",
        "    rewards = np.asarray(rewards, dtype=np.float64)\n",
        "    asset_returns = np.asarray(asset_returns, dtype=np.float64)\n",
        "    positions = np.asarray(positions, dtype=np.float64)\n",
        "    turnovers = np.asarray(turnovers, dtype=np.float64)\n",
        "    equity = np.asarray(equity, dtype=np.float64)\n",
        "\n",
        "    ann_factor = annualization_factor(cfg)\n",
        "    mean_reward = float(np.mean(rewards)) if rewards.size else float(\"nan\")\n",
        "    std_reward = float(np.std(rewards, ddof=1)) if rewards.size > 1 else float(\"nan\")\n",
        "\n",
        "    total_log_return = float(np.sum(rewards)) if rewards.size else float(\"nan\")\n",
        "    total_return = float(np.exp(total_log_return) - 1.0) if rewards.size else float(\"nan\")\n",
        "    annualized_return = float(np.exp(mean_reward * ann_factor) - 1.0) if rewards.size else float(\"nan\")\n",
        "    annualized_vol = float(std_reward * np.sqrt(ann_factor)) if rewards.size > 1 else float(\"nan\")\n",
        "    sharpe = safe_sharpe(mean_reward, std_reward, ann_factor)\n",
        "\n",
        "    downside = rewards[rewards < 0]\n",
        "    downside_std = float(np.std(downside, ddof=1)) if downside.size > 1 else float(\"nan\")\n",
        "    sortino = safe_sharpe(mean_reward, downside_std, ann_factor)\n",
        "\n",
        "    max_drawdown = compute_drawdown(equity)\n",
        "    calmar = float(annualized_return / abs(max_drawdown)) if max_drawdown < 0 else float(\"nan\")\n",
        "\n",
        "    win_rate = float(np.mean(rewards > 0)) if rewards.size else float(\"nan\")\n",
        "    avg_turnover = float(np.mean(turnovers)) if turnovers.size else float(\"nan\")\n",
        "    total_turnover = float(np.sum(turnovers)) if turnovers.size else float(\"nan\")\n",
        "    avg_position = float(np.mean(positions)) if positions.size else float(\"nan\")\n",
        "    pos_std = float(np.std(positions, ddof=1)) if positions.size > 1 else float(\"nan\")\n",
        "    abs_pos = float(np.mean(np.abs(positions))) if positions.size else float(\"nan\")\n",
        "\n",
        "    flat_mask = np.abs(positions) <= cfg.flat_threshold\n",
        "    long_mask = positions > cfg.flat_threshold\n",
        "    short_mask = positions < -cfg.flat_threshold\n",
        "    flat_frac = float(np.mean(flat_mask)) if positions.size else float(\"nan\")\n",
        "    long_frac = float(np.mean(long_mask)) if positions.size else float(\"nan\")\n",
        "    short_frac = float(np.mean(short_mask)) if positions.size else float(\"nan\")\n",
        "\n",
        "    trade_count = int(np.sum(np.abs(np.diff(positions)) > cfg.flat_threshold)) if positions.size > 1 else 0\n",
        "\n",
        "    bh_mean = float(np.mean(asset_returns)) if asset_returns.size else float(\"nan\")\n",
        "    bh_std = float(np.std(asset_returns, ddof=1)) if asset_returns.size > 1 else float(\"nan\")\n",
        "    bh_total_return = float(np.exp(np.sum(asset_returns)) - 1.0) if asset_returns.size else float(\"nan\")\n",
        "    bh_annualized_return = float(np.exp(bh_mean * ann_factor) - 1.0) if asset_returns.size else float(\"nan\")\n",
        "    bh_annualized_vol = float(bh_std * np.sqrt(ann_factor)) if asset_returns.size > 1 else float(\"nan\")\n",
        "    bh_sharpe = safe_sharpe(bh_mean, bh_std, ann_factor)\n",
        "\n",
        "    return {\n",
        "        \"asset_id\": asset_id,\n",
        "        \"steps\": int(n_steps),\n",
        "        \"total_return\": total_return,\n",
        "        \"annualized_return\": annualized_return,\n",
        "        \"annualized_volatility\": annualized_vol,\n",
        "        \"sharpe\": sharpe,\n",
        "        \"sortino\": sortino,\n",
        "        \"max_drawdown\": max_drawdown,\n",
        "        \"calmar\": calmar,\n",
        "        \"avg_reward\": mean_reward,\n",
        "        \"reward_volatility\": std_reward,\n",
        "        \"win_rate\": win_rate,\n",
        "        \"avg_turnover\": avg_turnover,\n",
        "        \"total_turnover\": total_turnover,\n",
        "        \"avg_position\": avg_position,\n",
        "        \"position_std\": pos_std,\n",
        "        \"avg_abs_position\": abs_pos,\n",
        "        \"long_frac\": long_frac,\n",
        "        \"short_frac\": short_frac,\n",
        "        \"flat_frac\": flat_frac,\n",
        "        \"trade_count\": trade_count,\n",
        "        \"bh_total_return\": bh_total_return,\n",
        "        \"bh_annualized_return\": bh_annualized_return,\n",
        "        \"bh_annualized_volatility\": bh_annualized_vol,\n",
        "        \"bh_sharpe\": bh_sharpe,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10 tickers from C:\\python\\koulu\\Gradu\\configs\\assets\\tickers0.txt\n",
            "tickers: ['CSCO', 'MRK', 'NKE', 'NVDA', 'DIS', 'AAPL', 'CAT', 'V', 'HON', 'AMZN']\n",
            "Loading evaluation dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ollik\\AppData\\Local\\Temp\\ipykernel_51592\\1710710934.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path, map_location=\"cpu\")\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "No assets found in the validation dataset.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m test_dataset = Dataset_Finance_MultiAsset(**dataset_kwargs)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m test_dataset.asset_ids:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo assets found in the validation dataset.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m MAX_ASSETS \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     40\u001b[39m     test_dataset.asset_ids = test_dataset.asset_ids[: \u001b[38;5;28mint\u001b[39m(MAX_ASSETS)]\n",
            "\u001b[31mRuntimeError\u001b[39m: No assets found in the validation dataset."
          ]
        }
      ],
      "source": [
        "\n",
        "ticker_list_path = paths_cfg.get(\"ticker_list_path\")\n",
        "if not ticker_list_path:\n",
        "    raise ValueError(\"Config is missing paths.ticker_list_path\")\n",
        "\n",
        "# Build dataset from the same config used in training\n",
        "tickers_path = PROJECT_ROOT / ticker_list_path\n",
        "tickers = load_tickers(str(tickers_path))\n",
        "if not tickers:\n",
        "    raise RuntimeError(f\"No tickers loaded from {tickers_path}\")\n",
        "\n",
        "dataset_kwargs = {\n",
        "    \"root_path\": dataset_cfg[\"root_path\"],\n",
        "    \"data_path\": dataset_cfg[\"data_path\"],\n",
        "    \"start_date\": dataset_cfg.get(\"start_date\"),\n",
        "    \"split\": \"val\",\n",
        "    \"size\": [dataset_cfg[\"context_len\"], dataset_cfg[\"target_len\"]],\n",
        "    \"use_time_features\": dataset_cfg.get(\"use_time_features\", True),\n",
        "    \"rolling_window\": dataset_cfg[\"rolling_window\"],\n",
        "    \"train_split\": dataset_cfg[\"train_split\"],\n",
        "    \"test_split\": dataset_cfg[\"test_split\"],\n",
        "    \"regular_hours_only\": dataset_cfg.get(\"regular_hours_only\", True),\n",
        "    \"timeframe\": dataset_cfg.get(\"timeframe\", \"15min\"),\n",
        "    \"tickers\": tickers,\n",
        "}\n",
        "\n",
        "asset_universe = load_asset_universe_from_checkpoint(JEPA_CHECKPOINT_PATH)\n",
        "if asset_universe:\n",
        "    dataset_kwargs[\"asset_universe\"] = asset_universe\n",
        "\n",
        "dataset_kwargs[\"tickers\"] = tickers\n",
        "print(f\"Loaded {len(tickers)} tickers from {tickers_path}\")\n",
        "print(f\"tickers: {tickers}\")\n",
        "\n",
        "print(\"Loading evaluation dataset...\")\n",
        "test_dataset = Dataset_Finance_MultiAsset(**dataset_kwargs)\n",
        "if not test_dataset.asset_ids:\n",
        "    raise RuntimeError(\"No assets found in the validation dataset.\")\n",
        "\n",
        "if MAX_ASSETS is not None:\n",
        "    test_dataset.asset_ids = test_dataset.asset_ids[: int(MAX_ASSETS)]\n",
        "\n",
        "print(f\"Assets to evaluate: {len(test_dataset.asset_ids)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_asset_ids = int(getattr(test_dataset, \"num_asset_ids\", len(test_dataset.asset_ids)))\n",
        "\n",
        "print(\"Loading JEPA model...\")\n",
        "jepa_model = build_jepa_model(device, num_assets=num_asset_ids)\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=JEPAAuxFeatureExtractor,\n",
        "    features_extractor_kwargs=dict(\n",
        "        jepa_model=jepa_model,\n",
        "        embedding_dim=jepa_cfg[\"d_model\"],\n",
        "        patch_len=jepa_cfg[\"patch_len\"],\n",
        "        patch_stride=jepa_cfg[\"patch_stride\"],\n",
        "        use_obs_targets=True,\n",
        "        target_len=test_dataset.pred_len,\n",
        "    ),\n",
        "    net_arch=dict(pi=[256, 256], vf=[256, 256]),\n",
        ")\n",
        "\n",
        "print(f\"Loading PPO model from {PPO_CHECKPOINT_PATH}...\")\n",
        "model = load_ppo_model(PPO_CHECKPOINT_PATH, device=device, policy_kwargs=policy_kwargs)\n",
        "model.policy.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_cfg = EvalConfig(\n",
        "    annual_trading_days=252,\n",
        "    regular_hours_only=dataset_kwargs.get(\"regular_hours_only\", True),\n",
        "    timeframe=dataset_kwargs.get(\"timeframe\", \"15min\"),\n",
        ")\n",
        "\n",
        "results: List[Dict[str, float]] = []\n",
        "for idx, asset_id in enumerate(test_dataset.asset_ids, start=1):\n",
        "    print(f\"[{idx}/{len(test_dataset.asset_ids)}] Evaluating {asset_id}...\")\n",
        "    metrics = eval_asset(model, test_dataset, asset_id, eval_cfg)\n",
        "    if metrics:\n",
        "        results.append(metrics)\n",
        "\n",
        "if not results:\n",
        "    raise RuntimeError(\"No evaluation results produced.\")\n",
        "\n",
        "df = pd.DataFrame(results).sort_values(\"asset_id\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save outputs\n",
        "os.makedirs(PROJECT_ROOT / log_root, exist_ok=True)\n",
        "metrics_path = PROJECT_ROOT / log_root / f\"{model_name}_test_metrics.csv\"\n",
        "summary_path = PROJECT_ROOT / log_root / f\"{model_name}_test_summary.csv\"\n",
        "\n",
        "df.to_csv(metrics_path, index=False)\n",
        "summary = df.drop(columns=[\"asset_id\"]).agg([\"mean\", \"median\"])\n",
        "summary.to_csv(summary_path)\n",
        "\n",
        "print(f\"Saved per-asset metrics to {metrics_path}\")\n",
        "print(f\"Saved summary to {summary_path}\")\n",
        "summary\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".graduenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
